{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "afb8ef4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\python\\lib\\site-packages\\tensorflow\\python\\compat\\v2_compat.py:101: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(action='ignore') \n",
    "%config Completer.use_jedi = False \n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "196c2a10",
   "metadata": {},
   "source": [
    "<img src=\"./images/MNIST1.png\" width=\"300\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c02dd36",
   "metadata": {},
   "source": [
    "MNIST 손글씨 실습을 위해서 케라스에서 제공하는 MNIST 데이터셋을 사용한다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e10cb330",
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c563a0bd",
   "metadata": {},
   "source": [
    "학습 데이터에는 총 60,000개의 샘플 데이터가 있고 테스트 데이터에는 10,000개의 샘플 데이터가 있다.  \n",
    "MNIST 데이터는 이미지 하나가 28개의 행과 28개의 열을 갖는 픽셀 데이터 이다. 각 픽셀은 흑백 사진과 같이 0부터 255까지의 그레이스케일을 가지고 있다.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d7f334d6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train.shape: (60000, 28, 28), y_train.shape: (60000,)\n",
      "x_test.shape: (10000, 28, 28), y_test.shape: (10000,)\n"
     ]
    }
   ],
   "source": [
    "print('x_train.shape: {}, y_train.shape: {}'.format(x_train.shape, y_train.shape)) # 학습 데이터\n",
    "print('x_test.shape: {}, y_test.shape: {}'.format(x_test.shape, y_test.shape)) # 테스트 데이터"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24fc26e9",
   "metadata": {},
   "source": [
    "학습 데이터를 학습 데이터(5만개)와 검증 데이터(1만개)로 분리한다.  \n",
    "학습 중간마다 검증 데이터로 모델의 성능을 측정하면 모델 학습이 제대로 진행되는지 검증 정확도를 알 수 있고 학습 정확도는 올라가는데 검증 정확도가 더 이상 올라가지 않거나 떨어질 경우 학습의 조기 종료를 구현할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4b151d13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train.shape: (50000, 28, 28), y_train.shape: (50000,)\n",
      "x_val.shape: (10000, 28, 28), y_val.shape: (10000,)\n",
      "x_test.shape: (10000, 28, 28), y_test.shape: (10000,)\n"
     ]
    }
   ],
   "source": [
    "# 검증 데이터로 사용하기 위해 학습 데이터에서 1만개를 분리한다.\n",
    "x_val = x_train[50000:] # 28행 28열로 구성된 검증 데이터\n",
    "x_train = x_train[:50000] # 28행 28열로 구성된 학습 데이터\n",
    "y_val = y_train[50000:] # 검증 데이터 실제값(레이블, 타겟, 클래스)\n",
    "y_train = y_train[:50000] # 검증 데이터\n",
    "print('x_train.shape: {}, y_train.shape: {}'.format(x_train.shape, y_train.shape)) # 학습 데이터\n",
    "print('x_val.shape: {}, y_val.shape: {}'.format(x_val.shape, y_val.shape)) # 검증 데이터\n",
    "print('x_test.shape: {}, y_test.shape: {}'.format(x_test.shape, y_test.shape)) # 테스트 데이터"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cde9762f",
   "metadata": {},
   "source": [
    "학습 데이터를 출력해보면 데이터가 0부터 255사이의 숫자(그레이스케일)로 구성된 것을 확인할 수 있다.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7f9f9979",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5 0 4 1 9 2 1 3 1 4]\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0189190  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0143247153  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0136247242 86  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0192252187  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0 62185 18  0  0  0  0 89236217 47  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0216253 60  0  0  0  0212255 81  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0206252 68  0  0  0 48242253 89  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0131251212 21  0  0 11167252197  5  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0 29232247 63  0  0  0153252226  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0 45219252143  0  0  0116249252103  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  4 96253255253200122  7 25201250158  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0 92252252253217252252200227252231  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0 87251247231 65 48189252252253252251227 35  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0190221 98  0  0  0 42196252253252252162  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0111 29  0  0  0  0 62239252 86 42 42 14  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0 15148253218  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0121252231 28  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0 31221251129  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0218252160  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0122252 82  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n"
     ]
    }
   ],
   "source": [
    "print(y_train[:10])\n",
    "for i in x_train[9]:\n",
    "    for j in i:\n",
    "        print('{:3d}'.format(j), end='')\n",
    "    print()      "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cce6c638",
   "metadata": {},
   "source": [
    "MNIST 데이터를 불러왔으니 다층 퍼셉트론의 입력값으로 돌아갈 수 있도록 넘파이의 reshape() 메소드를 사용해서 2차원 배열 형태의 데이터를 1차원 배열 형태로 변경한다.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7a0bc0b",
   "metadata": {},
   "source": [
    "<img src=\"./images/MNIST2.png\" width=\"1200\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4825f531",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train.shape: (50000, 28, 28)\n",
      "x_train.shape: (50000, 784)\n",
      "x_val.shape: (10000, 784)\n",
      "x_test.shape: (10000, 784)\n"
     ]
    }
   ],
   "source": [
    "# 28 * 28 픽셀의 단색 이미지이므로 데이터 형태를 784개의 1차원 배열 형태로 변환한다.\n",
    "print('x_train.shape: {}'.format(x_train.shape))\n",
    "# x_train = np.reshape(x_train, [50000, 784]) # 28행 28열로 구성된 학습 데이터를 784개의 1차원 배열 형태로 변환한다.\n",
    "# x_train = x_train.reshape(50000, 784)\n",
    "x_train = x_train.reshape(-1, 784)\n",
    "print('x_train.shape: {}'.format(x_train.shape))\n",
    "x_val = np.reshape(x_val, [10000, 784]) # 28행 28열로 구성된 검증 데이터를 784개의 1차원 배열 형태로 변환한다.\n",
    "print('x_val.shape: {}'.format(x_val.shape))\n",
    "x_test = np.reshape(x_test, [10000, 784]) # 28행 28열로 구성된 테스트 데이터를 784개의 1차원 배열 형태로 변환한다.\n",
    "print('x_test.shape: {}'.format(x_test.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29d61d2f",
   "metadata": {},
   "source": [
    "1차원으로 변경된 데이터를 그대로 다층 퍼셉트론에 입력해도 되지만 조금 더 효율적인 학습을 위해 데이터를 정규화 시킨다. 정규화는 모델의 학습 시간을 단축시키고 더 나은 성능을 보이게 하는 효과가 있다.  \n",
    "MNIST 손글씨 데이터의 모든 값들은 0부터 255사이의 범위 안에 있으므로 255로 나눠 모든 값들을 0부터 1사이의 값으로 정규화 한다.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "efc7895d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(type(x_train[0][0]), x_train[0][0]) # <class 'numpy.uint8'> 0\n",
    "# 255로 나눠서 0부터 1사이의 실수로 만들어야 하므로 astype() 메소드를 이용해서 정수를 실수로 변환한다.\n",
    "x_train = x_train.astype('float32')\n",
    "# print(type(x_train[0][0]), x_train[0][0]) # <class 'numpy.float32'> 0.0\n",
    "x_val = x_val.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "gray_scale = 255\n",
    "x_train /= gray_scale\n",
    "x_val /= gray_scale\n",
    "x_test /= gray_scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a35a77c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0, 1, 2, 3, 4, 5, 6, 7, 8, 9}\n"
     ]
    }
   ],
   "source": [
    "print(set(y_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f7e05dc",
   "metadata": {},
   "source": [
    "MNIST 손글씨 데이터 분류 모델은 0부터 9사이의 숫자로 분류하는 다중 모델이므로 손실 함수로 크로스 엔트로피를 사용한다.  \n",
    "크로스 엔트로피를 계산하기 위해 실제값(y_train, y_val, y_test)을 원-핫 인코딩(one-hot encoding)으로 변환한다.  \n",
    "원-핫 인코딩은 데이터를 수많은 0과 1개의 1로 데이터를 부별하는 인코딩 방식으로 0으로 이루어진 벡터 집합에 단 1개의 1의 값으로 해당 데이터를 구별하는 것을 말한다.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "94e2ba63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5 0 4 1 9]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      "[0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n"
     ]
    }
   ],
   "source": [
    "# to_categorical() 메소드로 데이터에 원-핫 인코딩을 적용할 수 있다.\n",
    "num_classes = 10\n",
    "print(y_train[:5])\n",
    "y_train = tf.keras.utils.to_categorical(y_train, num_classes)\n",
    "for i in y_train[:5]:\n",
    "    print(i)\n",
    "y_val = tf.keras.utils.to_categorical(y_val, num_classes)\n",
    "y_test = tf.keras.utils.to_categorical(y_test, num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da863103",
   "metadata": {},
   "source": [
    "입력 데이터는 784개의 숫자가 들어있는 1차원 배열이다.  \n",
    "784개의 입력을 받는 256개의 노드가 1번째 히든 레이어(h1)에 있고 1번째 히든 레이어의 출력값을 입력으로 받는 2번째 히든 레이어에는 128개의 노드가 있다.  \n",
    "2번째 히든 레이어에는 과대 적합을 방지하기 위해 10% 드롭 아웃을 적용한다.  \n",
    "3번째 히든 레이어에는 총 10개의 노드가 존재하며 이 10개의 노드값은 소프트 맥스를 통과해서 0부터 9사이에 해당되는 각 숫자의 확률을 의미한다. 소프트 맥스는 분류해야하는 정답지(레이블, 클래스, 타겟)의 총 개수를 k라고 할 때 k차원의 벡터를 입력받아 각 정답에 대한 확률을 추정한다.  \n",
    "소프트 맥스의 출력값과 실제값의 차이(오차)를 계산하기 위해 크로스 엔트로피를 손실 함수로 사용하고 손실 함수를 최소화 하기 위해서 Adam 옵티마이저(최적화 함수)를\n",
    "사용해서 역전파를 통해 모든 가중치 및 평향값을 최적화 한다.  \n",
    "\n",
    "최적화 함수 참고 사이트\n",
    "https://onevision.tistory.com/entry/Optimizer-%EC%9D%98-%EC%A2%85%EB%A5%98%EC%99%80-%ED%8A%B9%EC%84%B1-Momentum-RMSProp-Adam  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a01d2d6",
   "metadata": {},
   "source": [
    "<img src=\"./images/MNIST3.png\" width=\"1200\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "435e2656",
   "metadata": {},
   "source": [
    "소프트 맥스(Soft Max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1b95225c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 3 10  5]\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "a = tf.constant([3, 10, 5]) # 1차원 배열\n",
    "sess = tf.Session()\n",
    "print(sess.run(a))\n",
    "# argmax() 메소드는 배열에서 가장 큰 값을 찾아서 그 인덱스를 리턴한다.\n",
    "# a 배열에서 10이 가장 크기 때문에 결과는 10의 인덱스 1이 출력된다.\n",
    "print(sess.run(tf.argmax(a)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "326195d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 3 10  5]\n",
      " [ 4  5  6]\n",
      " [ 0  8  7]]\n",
      "[1 0 2]\n",
      "[1 0 2]\n",
      "[1 2 1]\n"
     ]
    }
   ],
   "source": [
    "b = tf.constant([[3, 10, 5], [4, 5, 6], [0, 8, 7]]) # 2차원 배열\n",
    "print(sess.run(b))\n",
    "# argmax() 메소드를 2차원에서 사용할 경우 2번째 인수를 지정해야 한다. 생략시 기본값은 0\n",
    "# 2번째 인수를 생략하거나 0을 사용하면 2차원 배열의 각 열에서 최대값의 인덱스를 리턴하고 2번재 인수에 1을 쓰면 2차원 배열의 각 행에서 최대값의 인덱스를 리턴한다.\n",
    "# b 배열에서 열 단위로 최대값을 계산하면 0번째 열의 최대값 4의 인덱스 1, 1번째 열의 최대값 10의 인덱스 0, 2번째 열의 최대값 7의 인덱스 2가 [1 0 2]와 같이 출력된다.\n",
    "print(sess.run(tf.argmax(b)))\n",
    "print(sess.run(tf.argmax(b,0)))\n",
    "# b 배열의 행 단위로 최대값을 계산하면 0번째 행의 최대값 10의 인덱스 1, 1번째 행의 최대값 6의 인덱스 2, 2번째 행의 최대값 8의 인덱스 1이 [1 2 1]과 같이 출력된다.\n",
    "print(sess.run(tf.argmax(b,1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1700d295",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1번째 차원이 None인 이유는 데이터 개수의 제약없이 입력받기 위해서이고 2번째 차원이 784인 것은 MNIST 손글씨 이미지의 크기가 28 * 28 픽셀 = 784 픽셀이기 때문이다. \n",
    "x = tf.placeholder(dtype=tf.float32, shape=[None, 784]) # 학습 데이터를 기억할 placeholder를 선언한다.\n",
    "y = tf.placeholder(dtype=tf.float32, shape=[None, 10]) # 실제값을 기억할 placeholder를 선언한다.\n",
    "keep_prob = tf.placeholder(dtype=tf.float32) # 드롭 아웃 적용 변수"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "038ad654",
   "metadata": {},
   "source": [
    "다층 퍼셉트론을 구현한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1ef1e2e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mlp(x):\n",
    "    # 히든 레이어1\n",
    "    w1 = tf.Variable(tf.random_uniform([784, 256]))\n",
    "    b1 = tf.Variable(tf.zeros([256]))\n",
    "    h1 = tf.nn.relu(tf.matmul(x, w1) + b1)\n",
    "    # 히든 레이어2\n",
    "    w2 = tf.Variable(tf.random_uniform([256, 128]))\n",
    "    b2 = tf.Variable(tf.zeros([128]))\n",
    "    h2 = tf.nn.relu(tf.matmul(h1, w2) + b2)\n",
    "    # 드롭아웃 적용, keep_prob만 사용했었는데 버전이 올라가면서 rate=1-keep_prob과 같은 형태로 사용해야 한다.\n",
    "    h2_drop = tf.nn.dropout(h2, rate=1-keep_prob)\n",
    "    # 히든 레이어3\n",
    "    w3 = tf.Variable(tf.random_uniform([128, 10]))\n",
    "    b3 = tf.Variable(tf.zeros([10]))\n",
    "    logit = tf.nn.relu(tf.matmul(h2_drop, w3) + b3)\n",
    "    return logit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61b7c44f",
   "metadata": {},
   "source": [
    "다층 퍼셉트론의 출력값을 logits(logistic + probit)로 정의한다. probit는 확률을 재는 단위를 의미한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0f26b44a",
   "metadata": {},
   "outputs": [],
   "source": [
    "logit = mlp(x)\n",
    "# logits와 실제값의 크로스 엔트로피를 손실 함수로 사용한다.\n",
    "loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits_v2(logits=logit, labels=y)\n",
    ")\n",
    "# Adam 옵티마이저를 사용해 모델을 최적화한다.\n",
    "# 모델의 최적화 과정은 모델의 예측값과 실제값의 차이를 줄여나가는 과정을 의미한다.\n",
    "train = tf.train.AdamOptimizer(learning_rate=0.01).minimize(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec43b8c9",
   "metadata": {},
   "source": [
    "조기 종료는 과대 적합을 피하고 충분한 학습을 하기 위해서 학습 중간마다 검증 데이터에 대한 정확도를 측정하면 학습 데이터에 대한 정확도는 계속 증가하는 반면에 검증 데이터에 대한 정확도가 점차 떨어질 경우 학습을 중지하는 것을 말한다.  \n",
    "매 epoch 마다 검증 데이터로 검증 정확도를 측정해서 검증 정확도가 5번 연속으로 검증 정확도의 최대값보다 높지 않을 경우 조기 종료를 수행한다.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0cb53996",
   "metadata": {},
   "outputs": [],
   "source": [
    "saver = tf.train.Saver() # 텐서플로우에서 학습 모델의 저장 및 로드에 사용할 변수를 선언한다.\n",
    "epoch_cnt = 300 # 조기 종료가 일어나지 않을 경우 최대 300번까지 반복해서 학습하도록 설정한다.\n",
    "batch_size = 1000 # 1번에 읽어서 처리할 데이터의 개수를 설정한다.\n",
    "iteration = len(x_train) // batch_size # batch_size에 따른 학습 횟수를 설정한다.\n",
    "earlystop_threshold = 5 # 검증 정확도가 검증 정확도의 최대값보다 5번 연속으로 높지 않을 경우 조기 종료하도록 설정한다.\n",
    "earlystop_cnt = 0 # 검증 정확도가 검증 정확도의 최대값보다 연속으로 높지 않은 검증 횟수를 세는 변수를 선언한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "903124cc",
   "metadata": {},
   "source": [
    "학습을 진행한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "31fbcd28",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-21-57bdd1e69e3f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     10\u001b[0m         \u001b[1;31m# 학습 데이터를 batch_size 개 만큼씩 나눠서 학습을 진행한다.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miteration\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m             \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss_op\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mx_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mend\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mend\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkeep_prob\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;36m0.9\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m             \u001b[1;31m# 학습할 데이터의 범위를 batch_size 만큼 이동시킨다.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m             \u001b[0mstart\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    966\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    967\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 968\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    969\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    970\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1189\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1190\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1191\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1192\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1193\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1367\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1368\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[1;32m-> 1369\u001b[1;33m                            run_metadata)\n\u001b[0m\u001b[0;32m   1370\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1371\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1373\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1374\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1375\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1376\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1377\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1358\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1359\u001b[0m       return self._call_tf_sessionrun(options, feed_dict, fetch_list,\n\u001b[1;32m-> 1360\u001b[1;33m                                       target_list, run_metadata)\n\u001b[0m\u001b[0;32m   1361\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1362\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[1;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[0;32m   1451\u001b[0m     return tf_session.TF_SessionRun_wrapper(self._session, options, feed_dict,\n\u001b[0;32m   1452\u001b[0m                                             \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1453\u001b[1;33m                                             run_metadata)\n\u001b[0m\u001b[0;32m   1454\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1455\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    prev_train_acc = 0.0 # 이전 학습 정확도를 기억할 변수를 선언한다.\n",
    "    max_val_acc = 0.0 # 검증 정확도의 최대값을 기억할 변수를 선언한다.\n",
    "    # 지정한 최대 epoch 만큼 학습한다. => 검증 정확도가 검증 정확도의 최대값보다 5번 연속으로 높지 않을 경우 조기 종료한다.\n",
    "    for epoch in range(epoch_cnt):\n",
    "        avg_loss = 0.0 # 손실값\n",
    "        start = 0 # 학습 시작 위치\n",
    "        end = batch_size # 학습 종료 위치\n",
    "        # 학습 데이터를 batch_size 개 만큼씩 나눠서 학습을 진행한다.\n",
    "        for i in range(iteration):\n",
    "            _, loss_op = sess.run([train, loss], feed_dict={x: x_train[start:end], y: y_train[start:end], keep_prob: 0.9})\n",
    "            # 학습할 데이터의 범위를 batch_size 만큼 이동시킨다.\n",
    "            start += batch_size\n",
    "            end += batch_size\n",
    "            # 크로스 엔트로피 손실 함수의 학습 손실값을 계산한다.\n",
    "            avg_loss += loss_op / iteration\n",
    "        # ===== for i\n",
    "        \n",
    "        # 모델 검증\n",
    "        predict = tf.nn.softmax(logits=logit) # 소프트 맥스 적용\n",
    "        correct_prediction = tf.equal(tf.argmax(predict, 1), tf.argmax(y, 1))\n",
    "        \n",
    "        # 정확도 계산\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, 'tloat'))\n",
    "        \n",
    "        # 학습 정확도 계산\n",
    "        # Session.run()과 Tensor.eval()의 차이\n",
    "        # t가 tensor 오브젝트라면 t.eval()은 sess.run(t)의 속기 표현이다. sess가 현재 디폴트 세션이 곳에서만 가능하다.\n",
    "        cur_train_acc = accuracy.eval({x: x_train, y: y_train, keep_prob=1.0})\n",
    "        \n",
    "        # 검증 정확도 계산\n",
    "        \n",
    "        \n",
    "        \n",
    "    # ===== for epoch\n",
    "    \n",
    "# ===== with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "060b6b14",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b20954ec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e963a070",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6364d1b0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
